{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this project, we have three datasets to analyze separately and then combined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets belong to twitter account WeRateDogs, and they represent their activity, such as number od tweets, retweets, visits, types of dogs as well as links of dog images and humorous descriptions.\n",
    "* First dataset is named **twitter-archive-enhanced** and we accessed it directly through download link in the description of project. It contains tweet ids, date and time of tweet posted, categories of dogs and their descrtiptions. I stored it into **tweet_df**.\n",
    "* Second dataset is called **image_predictions** and is hosted on Udacity's servers. I downloaded it programmatically using the Requests library and the URL as suggested in the project instructions. This file contains links of dogs pictures, as well as the predictions of dog breed. I stored it into **image_predictions**. \n",
    "* Third dataset was the most complicated to access, and it is **tweet.json** file. It contains data for every tweet from Twitter API. I did not have twitter account, and I did not want to create one, so I downloaded tweet.json file and I loaded it into the dataframe using the following steps:\n",
    "    * Creating the empty list of dictionaries\n",
    "    * Converting each json line to dict\n",
    "    * Appending the dicts to the list\n",
    "* This dataset contained a lot of useless information, columns with mostly NaN values, so I deleted a lot of the columns and used the rest for the analysis. I stored it into **tweet_metadata_df**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assesing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When opened, we can spot many tidiness and quality issues with the data by just looking briefly at each dataset.\n",
    "\n",
    "* **twitter-archive-enhanced** had some columns (retweet_status_id, retweet_status_user_id, retweet_status_timestamp,) almost empty-would be empty without retweets,  contains invalid dog names as well as some untrucated text (link) in the dog description column. Numerator and denomonator are also exceeding the limits introduced in project description.\n",
    "* **image_predictions** has some incorrect predictions for dog breed, probably because of bad pictures uploaded.\n",
    "* **tweet.json** file has a lot of unnecessary columns with useless data which I decided to delete, and I was left with only 4 columns, including number of retweets and likes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For **twitter-archive-enhanced** dataframe, I deleted the rows that had numerator smaller than 10 and greater than 100, as well as the ones with denominator different than 10. I replaced all the non-valid names with 'No_name', and separated 'text' column into dog description and link columns. I also categorized the sources of tweets and converted timestamp column to datetime type.\n",
    "* The biggest change for **image_predictions** was creating 'dog_breed column' and deleting all the other columns related to dog breed prediction.\n",
    "* I did not perform anything special on **tweet_json** file, just deleted unnecessary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I merged all the cleaned dataframes using inner join and stored it into dataframe named **twitter_archive_master_df** as suggested. I made a copy of a dataframe just to perform analysis on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
